Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.01s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.27s/it]
the `lang_code_to_id` attribute is deprecated. The logic is natively handled in the `tokenizer.adder_tokens_decoder` this attribute will be removed in `transformers` v4.38
/zhome/0a/b/138401/Desktop/Apart/Multi-Lingual-Jailbreaks/env/lib/python3.10/site-packages/fairseq/models/transformer/transformer_encoder.py:281: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)
  x = torch._nested_tensor_from_mask(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:13,  4.66s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.75s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:16<00:05,  5.81s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.30s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.63s/it]
Traceback (most recent call last):
  File "/zhome/0a/b/138401/Desktop/Apart/Multi-Lingual-Jailbreaks/pipeline.py", line 204, in <module>
    answer_pipeline(LLM,language,translation_model)
  File "/zhome/0a/b/138401/Desktop/Apart/Multi-Lingual-Jailbreaks/pipeline.py", line 95, in answer_pipeline
    model = Llama.AutoModel(LLM_name)
  File "/zhome/0a/b/138401/Desktop/Apart/Multi-Lingual-Jailbreaks/Llama.py", line 15, in __init__
    self.model.to(self.device)
  File "/zhome/0a/b/138401/Desktop/Apart/Multi-Lingual-Jailbreaks/env/lib/python3.10/site-packages/transformers/modeling_utils.py", line 2724, in to
    return super().to(*args, **kwargs)
  File "/zhome/0a/b/138401/Desktop/Apart/Multi-Lingual-Jailbreaks/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1173, in to
    return self._apply(convert)
  File "/zhome/0a/b/138401/Desktop/Apart/Multi-Lingual-Jailbreaks/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 779, in _apply
    module._apply(fn)
  File "/zhome/0a/b/138401/Desktop/Apart/Multi-Lingual-Jailbreaks/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 779, in _apply
    module._apply(fn)
  File "/zhome/0a/b/138401/Desktop/Apart/Multi-Lingual-Jailbreaks/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 804, in _apply
    param_applied = fn(param)
  File "/zhome/0a/b/138401/Desktop/Apart/Multi-Lingual-Jailbreaks/env/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1159, in convert
    return t.to(
  File "/zhome/0a/b/138401/Desktop/Apart/Multi-Lingual-Jailbreaks/env/lib/python3.10/site-packages/torch/cuda/__init__.py", line 293, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
